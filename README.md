# Real-Time Dynamic Pricing System with Fault-Tolerant Stream Processing

## Abstract

A production-grade, distributed dynamic pricing system for ride-sharing platforms, demonstrating fault-tolerant stream processing with Apache Flink on Kubernetes. The system processes ~250 events/second across 16 geographic zones, calculating surge multipliers in real-time using windowed aggregations with exactly-once semantics.

**Key Features:**
- ‚ö° Real-time pricing with <200ms end-to-end latency
- üîÑ Fault-tolerant stream processing (recovery times: 41-167s)
- üéØ Deterministic testing framework for reproducible research
- ‚ò∏Ô∏è Cloud-native deployment with Kubernetes High Availability
- üìä Disaster recovery validated (100% system loss ‚Üí 167s automatic recovery)

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [System Architecture](#2-system-architecture)
3. [Mathematical Model](#3-mathematical-model)
4. [Installation & Setup](#4-installation--setup)
5. [Deployment Options](#5-deployment-options)
6. [Fault Tolerance](#6-fault-tolerance)
7. [API Reference](#7-api-reference)
8. [Performance](#8-performance)
9. [Appendices](#9-appendices)

---

## 1. Introduction

### 1.1 Problem Statement

On-demand ride-sharing platforms face the challenge of dynamically balancing supply (available drivers) and demand (ride requests) in real-time. When demand exceeds supply, prices must adjust to:
- Incentivize more drivers to come online
- Manage passenger expectations
- Optimize platform revenue and utilization

Traditional batch-processing approaches introduce latency unsuitable for real-time pricing decisions. This system addresses this challenge through event stream processing.

### 1.2 Objectives

1. Implement a scalable, fault-tolerant real-time pricing engine
2. Demonstrate stream processing patterns using Apache Flink
3. Evaluate system performance under various load conditions
4. Provide reproducible experimental framework for academic research

### 1.3 Key Contributions

- **Fault-Tolerant Stream Processing**: Comprehensive evaluation across 6 failure scenarios with automatic recovery
- **Cloud-Native Architecture**: Kubernetes deployment with HA, achieving 99.9%+ availability
- **Configuration-Sensitive Recovery**: Demonstrates restart strategy tuning impact on fault tolerance (80% ‚Üí 100% auto-recovery)
- **Disaster Recovery**: Validated automatic recovery from complete system failure (167s from 100% pod loss)
- **Reproducible Research Framework**: Deterministic event generation with comprehensive evidence collection

---

## 2. System Architecture

### 2.1 High-Level Architecture

```mermaid
graph TB
    subgraph Client["Client Layer"]
        Dashboard["Web Dashboard<br/>(Real-time UI)"]
        MobileApp["REST Clients<br/>(Mobile/Web Apps)"]
    end

    subgraph Application["Application Layer"]
        API["Pricing API<br/>(Spring Boot)<br/>‚Ä¢ REST Endpoints<br/>‚Ä¢ Server-Sent Events<br/>‚Ä¢ Kafka Consumer"]
        DB[(PostgreSQL<br/>Historical Data)]
        KafkaBroker["Apache Kafka<br/>Message Broker"]
    end

    subgraph StreamProcessing["Stream Processing Layer"]
        Flink["Apache Flink Job<br/>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ<br/>1. Event Normalization<br/>2. Zone-based Keying<br/>3. Tumbling Window (3s)<br/>4. Supply/Demand Aggregation<br/>5. Surge Calculation<br/>6. Price Update Publishing"]
    end

    subgraph Topics["Kafka Topics"]
        T1[ride-requests]
        T2[driver-heartbeats]
        T3[price-updates]
    end

    subgraph EventGen["Event Generation Layer"]
        Generator["Event Generator<br/>(Spring Boot)<br/>‚Ä¢ Ride Requests (Poisson Œª=10.0)<br/>‚Ä¢ Driver Heartbeats (1s interval)<br/>‚Ä¢ Deterministic Experiments"]
    end

    Dashboard -->|HTTP/SSE| API
    MobileApp -->|HTTP/REST| API
    API -->|Persist| DB
    API -.->|Consume| KafkaBroker
    KafkaBroker -->|Subscribe| T3
    T3 -->|Publish| Flink
    Flink -->|Subscribe| T1
    Flink -->|Subscribe| T2
    T1 -->|Publish| Generator
    T2 -->|Publish| Generator
    Flink -->|Publish| T3

    style Client fill:#e1f5ff
    style Application fill:#fff4e1
    style StreamProcessing fill:#ffe1f5
    style EventGen fill:#e1ffe1
    style Topics fill:#f0f0f0
```


### 2.2 Stream Processing Pipeline

```mermaid
graph LR
    subgraph Input["Kafka Sources"]
        RR[ride-requests<br/>~160 events/s]
        DH[driver-heartbeats<br/>~96 events/s]
    end
    
    subgraph Processing["Flink Pipeline"]
        direction TB
        N1[Normalize<br/>Events]
        K1[KeyBy<br/>zone_id]
        W1[Window<br/>3s tumbling]
        A1[Aggregate<br/>Count demand & supply]
        C1[Calculate<br/>Surge = f‚Å°ratio‚Å°]
    end
    
    subgraph Output["Kafka Sink"]
        PU[price-updates<br/>~5 events/s]
    end
    
    RR --> N1
    DH --> N1
    N1 --> K1
    K1 --> W1
    W1 --> A1
    A1 --> C1
    C1 --> PU
    
    style Input fill:#e1f5ff
    style Processing fill:#ffe1f5
    style Output fill:#e1ffe1
```

**Pipeline Stages:**
1. **Normalization** ‚Üí Common event format
2. **Keying** ‚Üí Partition by zone (16 partitions)
3. **Windowing** ‚Üí 3-second tumbling windows
4. **Aggregation** ‚Üí Count requests and drivers per zone
5. **Calculation** ‚Üí Apply surge function: \( M = f(D/S) \)

**Throughput:** 256 events/s input ‚Üí 5.3 events/s output (16 zones √∑ 3s windows)

### 2.3 Technology Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Stream Processing** | Apache Flink | 1.17.2 | Exactly-once semantics, stateful windows, fault tolerance |
| **Message Broker** | Apache Kafka | 3.6.0 | High throughput, durability, replay capability |
| **API Layer** | Spring Boot | 3.2.0 | REST + SSE, Kafka integration, production-ready |
| **Storage** | PostgreSQL | 16 | Time-series data, ACID compliance |
| **Orchestration** | Kubernetes | 1.28+ | HA deployment, auto-scaling, self-healing |

---

## 3. Mathematical Model

### 3.1 Supply-Demand Ratio

For each zone \( z \) in time window \( w \):

$$R_{z,w} = \frac{D_{z,w}}{S_{z,w} + \epsilon}$$

**Where:**
- \( R_{z,w} \) = Demand/supply ratio for zone \( z \)
- \( D_{z,w} \) = Ride requests in window \( w \)
- \( S_{z,w} \) = Available drivers in window \( w \)
- \( \epsilon = 1 \) = Small constant to prevent division by zero

### 3.2 Surge Multiplier Function

Piecewise linear function with progressive steepness:

$$M(R) = \begin{cases}
1.0 & R \leq 0.5 \text{ (oversupply)} \\
1.0 + (R - 0.5) & 0.5 < R \leq 0.8 \\
1.3 + 1.75(R - 0.8) & 0.8 < R \leq 1.2 \\
2.0 + 2.5(R - 1.2) & 1.2 < R \leq 2.0 \\
4.0 + 1.5(R - 2.0) & 2.0 < R \leq 3.0 \\
\min(7.0, 5.5 + 0.75(R - 3.0)) & R > 3.0 \text{ (capped)}
\end{cases}$$

**Function Properties:**
- **Domain:** \( R \in [0, \infty) \)
- **Range:** \( M \in [1.0, 7.0] \) (capped at 7√ó)
- **Continuity:** Continuous at all breakpoints
- **Monotonicity:** Strictly increasing for \( R > 0.5 \)

**Surge Behavior:**

| Ratio \( R \) | Multiplier \( M(R) \) | Market Condition |
|---------------|----------------------|------------------|
| \( R \leq 0.5 \) | 1.0√ó (no surge) | Oversupply |
| \( R = 1.0 \) | 1.55√ó | Balanced |
| \( R = 1.5 \) | 2.75√ó | High demand |
| \( R = 2.5 \) | 4.75√ó | Critical |
| \( R \geq 9.33 \) | 7.0√ó (cap) | Extreme shortage |

### 3.3 Windowing Semantics

**Tumbling Windows:** Non-overlapping, fixed-duration aggregation intervals

$$W_i = [t_0 + i\delta,\, t_0 + (i+1)\delta) \quad \text{where } \delta = 3\text{s}$$

```mermaid
gantt
    title Tumbling Window Timeline (Œ¥ = 3s)
    dateFormat X
    axisFormat %Ls
    
    section Windows
    W‚ÇÄ (0-3s)   :0, 3000
    W‚ÇÅ (3-6s)   :3000, 3000
    W‚ÇÇ (6-9s)   :6000, 3000
    W‚ÇÉ (9-12s)  :9000, 3000
```

**Window Properties:**
- **Size:** \( \delta = 3 \) seconds
- **Overlap:** 0 seconds (tumbling)
- **Trigger:** On window close (processing time)
- **Output:** 1 aggregate per zone per window
- **Latency:** Max \( \delta \) seconds (window duration)

**Rationale:** 3-second windows balance real-time responsiveness with stable aggregations. Shorter windows (1s) increase volatility; longer windows (10s+) reduce responsiveness.

---

## 4. Installation & Setup

### 4.1 Prerequisites

- **Java Development Kit (JDK) 17 or higher**
  ```bash
  java -version  # Verify installation
  ```

- **Docker Desktop** with Docker Compose
  ```bash
  docker --version
  docker compose version
  ```

- **Python 3.8+** (for web dashboard)
  ```bash
  python3 --version
  ```

- **Available Ports**: 3000, 5432, 8080, 8081, 8082, 9093, 19092

### 4.2 Environment Configuration (IMPORTANT)

#### Cross-Platform Compatibility

This system supports **Docker Desktop**, **OrbStack**, **Rancher Desktop**, **Podman**, and works on **macOS**, **Windows**, and **Linux**. The Kafka broker needs to be accessible from both Docker Compose services and Kubernetes (Flink).

**Automatic Configuration (Recommended):**

```bash
# Detect your environment and auto-configure
./scripts/utils.sh detect-env
```

This script will:
- Detect your operating system (macOS, Linux, Windows)
- Identify your container runtime (Docker Desktop, OrbStack, Podman)
- Create a `.env` file with the correct `KAFKA_EXTERNAL_HOST` setting

**Manual Configuration (if needed):**

If auto-detection doesn't work, create a `.env` file in the project root:

```bash
# For macOS (Docker Desktop or OrbStack)
KAFKA_EXTERNAL_HOST=host.docker.internal

# For Windows (Docker Desktop)
KAFKA_EXTERNAL_HOST=host.docker.internal

# For Linux (Docker)
KAFKA_EXTERNAL_HOST=172.17.0.1  # Or your Docker bridge IP

# For Linux (Podman)
KAFKA_EXTERNAL_HOST=host.containers.internal
```

**Why This Matters:**

Different container runtimes use different hostnames to access the host machine:
- **Docker Desktop (Mac/Windows)**: `host.docker.internal` ‚úÖ
- **OrbStack (Mac)**: `host.docker.internal` ‚úÖ  
- **Podman (Linux)**: `host.containers.internal` ‚úÖ
- **Docker (Linux)**: Bridge IP (typically `172.17.0.1`) ‚úÖ

The Flink job running in Kubernetes needs to connect to Kafka running in Docker Compose. Without the correct hostname, the Flink job will fail with DNS resolution errors.

### 4.3 Quick Start (Local Development)

#### Step 1: Clone Repository
```bash
git clone <repository-url>
cd dynamic-pricing
```

#### Step 2: Configure Environment (First Time Only)
```bash
# Auto-detect and configure for your platform
./scripts/utils.sh detect-env

# This creates a .env file with the correct settings
# Review the settings if needed
cat .env
```

> **Note**: This step is crucial for cross-platform compatibility. It ensures Kubernetes (Flink) can reach Kafka regardless of whether you're using Docker Desktop, OrbStack, or running on Linux.

#### Step 3: Start Infrastructure
```bash
./scripts/dev.sh start
```

This script performs the following operations:
1. Validates Docker availability
2. Terminates any existing processes
3. Initializes Kafka and PostgreSQL containers
4. Creates required Kafka topics
5. Builds all Java applications
6. Starts services in dependency order

**Expected startup time**: 1-2 minutes

#### Step 4: Verify System Health
```bash
./scripts/dev.sh status
```

Expected output:
- ‚úÖ All Docker containers running
- ‚úÖ Kafka topics created
- ‚úÖ Database accessible
- ‚úÖ All services responding

#### Step 5: Access Interfaces

| Interface | URL | Purpose |
|-----------|-----|---------|
| Web Dashboard | http://localhost:3000 | Real-time pricing visualization |
| Kafka UI | http://localhost:8080 | Message broker monitoring |
| Pricing API | http://localhost:8081/api/v1/health | REST API health check |
| Event Generator | http://localhost:8082/actuator/health | Event simulation metrics |

### 4.4 System Shutdown

```bash
./scripts/stop.sh
```

Gracefully terminates all services and Docker containers.

---

## 5. Deployment Options

### 5.1 Local Development (Docker Compose)

**Use for:** Development, testing, debugging

```bash
./scripts/dev.sh start  # Start all services locally
./scripts/dev.sh status # Check health
./scripts/dev.sh stop   # Shutdown
```

**Architecture:** All services run in Docker containers with Flink in local mode.

### 5.2 Kubernetes Deployment (Production-Ready)

**Use for:** Production, HA testing, fault tolerance evaluation

#### Setup (One-Time):
```bash
# Install Flink operator, MinIO, namespaces
./scripts/k8s.sh setup
```

#### Deploy:
```bash
# Build and deploy Flink job to Kubernetes
./scripts/k8s.sh deploy

# Access Flink UI
./scripts/k8s.sh ports start
# Visit http://localhost:9081
```


**Kubernetes Architecture:**

```mermaid
graph TB
    subgraph K8s["Kubernetes Cluster"]
        subgraph FlinkNS["Namespace: flink"]
            JM1[JobManager 1<br/>Active<br/>1GB RAM]
            JM2[JobManager 2<br/>Standby<br/>1GB RAM]
            TM1[TaskManager 1<br/>4 slots<br/>1GB RAM]
            TM2[TaskManager 2<br/>4 slots<br/>1GB RAM]
        end
        
        subgraph MinioNS["Namespace: minio"]
            S3[MinIO<br/>S3-compatible<br/>Checkpoints]
        end
        
        subgraph OpNS["Namespace: flink-operator"]
            OP[Flink Operator<br/>Reconciliation]
        end
    end
    
    JM1 -->|Checkpoints| S3
    JM2 -->|Checkpoints| S3
    TM1 -->|Tasks| JM1
    TM2 -->|Tasks| JM1
    OP -->|Manages| JM1
    OP -->|Manages| JM2
    OP -->|Manages| TM1
    OP -->|Manages| TM2
    
    style JM1 fill:#90EE90
    style JM2 fill:#FFD700
    style TM1 fill:#87CEEB
    style TM2 fill:#87CEEB
    style S3 fill:#FFB6C1
    style OP fill:#DDA0DD
```

**High Availability Features:**
- **2 JobManagers:** Active-standby with leader election (recovery: 71s)
- **2 TaskManagers:** Parallel workers with task redistribution (recovery: 55s)
- **S3 Checkpoints:** Every 10 seconds for exactly-once guarantees
- **Auto-healing:** Kubernetes recreates failed pods automatically

### 5.3 Deployment Comparison

| Feature | Local (Docker) | Kubernetes |
|---------|----------------|------------|
| **Setup Complexity** | Low (1 command) | Medium (3 commands) |
| **High Availability** | ‚ùå No | ‚úÖ Yes (2 JMs) |
| **Fault Tolerance** | Limited | Complete |
| **Scalability** | Single node | Multi-node |
| **Resource Usage** | ~3GB | ~4-6GB |
| **Use Case** | Development | Production/Research |
| **Recovery Time** | Manual restart | 41-167s automatic |

---

## 6. Fault Tolerance

### 6.1 Fault Tolerance Architecture

```mermaid
graph TD
    subgraph FT["Three-Layer Fault Tolerance"]
        subgraph L1["Layer 1: Infrastructure (Kubernetes)"]
            P1[Liveness Probes<br/>Failure Detection]
            P2[ReplicaSets<br/>Pod Recreation]
            P3[Service Discovery<br/>Network Healing]
        end
        
        subgraph L2["Layer 2: Orchestration (Flink Operator)"]
            O1[Reconciliation<br/>Desired State]
            O2[Lifecycle Mgmt<br/>Job Transitions]
        end
        
        subgraph L3["Layer 3: Application (Flink)"]
            A1[Checkpointing<br/>Every 10s ‚Üí S3]
            A2[Leader Election<br/>K8s Leases]
            A3[Restart Strategy<br/>10 attempts √ó 10s]
        end
    end
    
    P1 --> P2
    P2 --> O1
    O1 --> A1
    A1 --> A2
    A2 --> A3
    
    style L1 fill:#E8F5E9
    style L2 fill:#E3F2FD
    style L3 fill:#FFF3E0
```

### 6.2 Comprehensive Fault Tolerance Evaluation

**6 Failure Scenarios Tested:**

| Scenario | Failure Type | Recovery Time | Auto? | Key Finding |
|----------|--------------|---------------|-------|-------------|
| **S1** | Standby JobManager | 41s | ‚úÖ | Zero downtime |
| **S2** | Active JobManager | 71s | ‚úÖ | HA failover works |
| **S3** | Both JobManagers | 103s | ‚úÖ | Cold bootstrap from S3 |
| **S4** | Single TaskManager | 55s | ‚úÖ | Task redistribution |
| **S5** | Both TaskManagers | 72s* | ‚úÖ | Config-dependent |
| **S6** | **All 4 Pods** | **167s** | ‚úÖ | **Disaster recovery** |

\* Requires 10 retry attempts (default 3 fails)

**Recovery Formula:**

$$T_{recovery} \approx T_{detect} + T_{provision} + T_{elect} + T_{restore}$$

**Where:**
- \( T_{detect} \) = Failure detection time ‚âà 5s (probe intervals)
- \( T_{provision} \) = Pod creation + initialization ‚âà 10-60s
- \( T_{elect} \) = Leader election (if needed) ‚âà 0-40s
- \( T_{restore} \) = Checkpoint restore from S3 ‚âà 0-45s

**Example (S2 - Active JM Failure):**
$$T_{recovery} = 5s + 30s + 30s + 30s = 95s \text{ (measured: 71s, some parallelism)}$$

### 6.3 Restart Strategy Configuration

**Critical Parameter:**

$$\text{Required Attempts} \geq \left\lceil \frac{T_{pod\_ready}}{T_{delay}} \right\rceil + \text{margin}$$

**For Kubernetes:**
- \( T_{pod\_ready} \approx 60s \) (typical pod initialization)
- \( T_{delay} = 10s \) (configured)
- \( \text{margin} = 2 \) (safety buffer)
- **Required:** \( \lceil 60/10 \rceil + 2 = 8 \) attempts minimum
- **Configured:** 10 attempts (enables S5, S6 auto-recovery)

**Configuration:**
```yaml
# In flink-deployment.yaml
flinkConfiguration:
  restart-strategy: fixed-delay
  restart-strategy.fixed-delay.attempts: "10"  # Critical for complete failures
  restart-strategy.fixed-delay.delay: "10s"
```

### 6.4 Running Fault Tolerance Tests

```bash
# Individual scenarios
./scripts/k8s.sh test-ft 1  # Standby JM failure
./scripts/k8s.sh test-ft 2  # Active JM failure (HA failover)
./scripts/k8s.sh test-ft 6  # Complete system failure

# All scenarios (20-25 minutes)
./scripts/k8s.sh test-ft all

# Evidence collected in: fault-tolerance-evidence/
```

**Evidence Generated:**
- Pod status snapshots (baseline, during, post-recovery)
- Kubernetes events timeline
- Checkpoint metadata (restore points, state size)
- Component logs (JobManager, TaskManager, Operator)
- Recovery time measurements

**Academic Use:** Complete evidence package for fault tolerance research papers.

---

## 7. System Components

### 7.1 Event Generator

**Purpose**: Simulates real-world ride-sharing platform events

**Configuration** (`services/event-generator/src/main/resources/application-local.yml`):
```yaml
app:
  zones: 16                      # Number of geographic zones
  drivers-per-zone: 6            # Drivers per zone
  heartbeat-interval-ms: 1000    # Driver location update frequency (1s)
  rides-lambda: 10.0             # Poisson parameter for ride arrival rate
```

**Event Types**:

1. **Ride Requests** (Poisson arrival, Œª=10.0 per zone)
   ```json
   {
     "requestId": "uuid",
     "zoneId": 1,
     "timestamp": 1698234567890,
     "eventId": "deterministic-hash"
   }
   ```

2. **Driver Heartbeats** (Periodic, every 1 second)
   ```json
   {
     "driverId": "driver-uuid",
     "zoneId": 1,
     "status": "AVAILABLE",  // AVAILABLE or BUSY
     "timestamp": 1698234567890
   }
   ```

### 7.2 Flink Processing Job

**Purpose**: Real-time stream processing and surge calculation

**Configuration** (`flink-pricing-job/src/main/java/com/pricing/flink/PricingJobMain.java`):
```java
private static final int PROCESSING_TIME_WINDOW = 3;  // 3-second windows
private static final int PARALLELISM = 4;              // Processing parallelism
private static final int CHECKPOINT_INTERVAL = 15000;  // Checkpoint frequency
```

**Processing Pipeline**:

1. **Event Normalization**: Converts different event types to common format
2. **Zone-based Keying**: Partitions events by geographic zone
3. **Windowed Aggregation**: Counts rides and drivers per 3-second window
4. **Surge Calculation**: Applies pricing function to supply-demand ratio
5. **Price Publishing**: Emits price updates to Kafka

**Fault Tolerance**: Periodic checkpointing ensures exactly-once processing semantics

### 7.3 Pricing API

**Purpose**: REST API and real-time streaming interface

**Endpoints**:

#### GET `/api/v1/zones/{zoneId}/price`
Returns current surge pricing for specified zone.

**Response**:
```json
{
  "zoneId": 1,
  "surgeMultiplier": 2.5,
  "demand": 15,
  "supply": 6,
  "ratio": 2.5,
  "updatedAt": "2025-10-25T19:30:00Z"
}
```

#### GET `/api/v1/zones/all/stream`
Server-Sent Events (SSE) stream for all zones (recommended for dashboards).

**Event Format**:
```
event: zone-update
data: {"zone_id":1,"surge_multiplier":2.5,"demand":15,"supply":6,...}
```

#### GET `/api/v1/zones/{zoneId}/stream`
SSE stream for individual zone.

#### GET `/api/v1/zones/{zoneId}/history`
Historical pricing data within time range.

**Query Parameters**:
- `from`: Start timestamp (epoch milliseconds)
- `to`: End timestamp (epoch milliseconds)

#### POST `/api/v1/quote`
Calculate fare estimate based on distance and duration.

**Request**:
```json
{
  "originZoneId": 1,
  "estDistanceKm": 5.5,
  "estDurationMin": 15
}
```

**Response**:
```json
{
  "baseFare": 8.50,
  "distanceFare": 11.00,
  "timeFare": 7.50,
  "surgeMultiplier": 2.5,
  "totalFare": 67.50,
  "zoneId": 1
}
```

### 7.4 Web Dashboard

**Purpose**: Real-time visualization of pricing dynamics

**Features**:
- Zone-wise surge multiplier display
- Supply and demand metrics per zone
- Real-time updates via Server-Sent Events
- Filter for surge-only zones
- Responsive design for various screen sizes

**Technology**: Pure HTML5/JavaScript with SSE API

---

## 8. API Reference

### 8.1 REST Endpoints Summary

| Method | Endpoint | Description | Response Type |
|--------|----------|-------------|---------------|
| GET | `/api/v1/zones/{id}/price` | Current price for zone | JSON |
| GET | `/api/v1/zones/{id}/stream` | Real-time updates (SSE) | text/event-stream |
| GET | `/api/v1/zones/all/stream` | All zones real-time | text/event-stream |
| GET | `/api/v1/zones/{id}/history` | Historical pricing | JSON |
| POST | `/api/v1/quote` | Fare calculation | JSON |
| GET | `/api/v1/health` | Service health | JSON |

### 8.2 Example Usage

#### cURL Examples

```bash
# Get current price
curl -X GET http://localhost:8081/api/v1/zones/1/price | jq

# Stream real-time updates (all zones)
curl -N http://localhost:8081/api/v1/zones/all/stream

# Get historical data
curl -X GET "http://localhost:8081/api/v1/zones/1/history?from=1698234000000&to=1698237600000" | jq

# Calculate fare quote
curl -X POST http://localhost:8081/api/v1/quote \
  -H "Content-Type: application/json" \
  -d '{
    "originZoneId": 1,
    "estDistanceKm": 5.5,
    "estDurationMin": 15
  }' | jq
```

---

## 9. Performance

### 9.1 Throughput Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| Events/second | ~10,000 | With default configuration |
| Window size | 3 seconds | Tumbling windows |
| Processing latency | <100ms | p95 latency |
| End-to-end latency | <200ms | Event to dashboard |
| Flink parallelism | 4 | Configurable |

### 9.2 Event Generation Rates

**Per Zone**:
- Driver heartbeats: ~6 events/second (6 drivers √ó 1Hz)
- Ride requests: ~10 events/second (Poisson Œª=10.0)

**System-wide** (16 zones):
- Total heartbeats: ~96 events/second
- Total ride requests: ~160 events/second
- **Combined**: ~256 events/second

### 9.3 Resource Utilization

**Development Configuration**:
- JVM Heap (Flink): 1GB
- JVM Heap (Spring Boot): 512MB each
- PostgreSQL: 256MB
- Kafka: 512MB
- Total Memory: ~3GB

**Recommended Production**:
- Flink TaskManagers: 2GB+ per instance
- Kafka: 4GB+ heap
- PostgreSQL: 1GB+ with tuning

### 9.4 Scalability

**Horizontal Scaling**:
- Flink parallelism can be increased based on topic partitions
- Kafka partitions match zone count (16) for optimal distribution
- Multiple Pricing API instances can share consumer group

**Bottlenecks**:
- Database writes: Consider write buffering or time-series database
- SSE connections: Consider Redis pub/sub for massive scale

---

## 10. Appendices

### Appendix A: Zone Configuration

The system models 16 geographic zones across 4 zone types:

| Zones | Type | Description | Base Fare | Distance Rate | Time Rate |
|-------|------|-------------|-----------|---------------|-----------|
| 1-4 | DOWNTOWN | Business districts | R$ 8.00-10.00 | R$ 2.50/km | R$ 0.60/min |
| 5-8 | URBAN | Residential/commercial | R$ 5.50-7.00 | R$ 2.00/km | R$ 0.50/min |
| 9-12 | SUBURBAN | Residential suburbs | R$ 4.00-5.00 | R$ 1.80/km | R$ 0.40/min |
| 13-16 | AIRPORT | Airport & special routes | R$ 12.00-15.00 | R$ 3.00/km | R$ 0.70/min |

### Appendix B: Database Schema

**zone_price_snapshot** - Current pricing state per zone

| Column | Type | Description |
|--------|------|-------------|
| zone_id | INTEGER | Primary key, zone identifier |
| surge_multiplier | DECIMAL(4,2) | Current surge (1.0-7.0) |
| demand | INTEGER | Ride requests in last window |
| supply | INTEGER | Available drivers in last window |
| ratio | DECIMAL(10,4) | Demand/supply ratio |
| updated_at | TIMESTAMPTZ | Last update timestamp |

**zone_window_metrics** - Historical time-series data

| Column | Type | Description |
|--------|------|-------------|
| zone_id | INTEGER | Zone identifier |
| window_start | TIMESTAMPTZ | Window start (PK) |
| demand | INTEGER | Ride count |
| supply | INTEGER | Driver count |
| surge_multiplier | DECIMAL(4,2) | Calculated surge |

**fare_config** - Base fare configuration per zone

| Column | Type | Description |
|--------|------|-------------|
| zone_id | INTEGER | Primary key |
| base_fare | DECIMAL(8,2) | Base charge (R$) |
| distance_rate | DECIMAL(6,2) | Per kilometer rate |
| time_rate | DECIMAL(6,2) | Per minute rate |
| zone_type | VARCHAR(20) | Zone classification |

### Appendix C: Fault Tolerance Test Results

**Complete Test Matrix:**

```mermaid
graph LR
    subgraph Coordinator["Coordinator Tier"]
        S1[S1: Standby<br/>41s]
        S2[S2: Active<br/>71s]
        S3[S3: Both<br/>103s]
    end
    
    subgraph Worker["Worker Tier"]
        S4[S4: Single<br/>55s]
        S5[S5: Both<br/>72s*]
    end
    
    subgraph Complete["Complete System"]
        S6[S6: All Pods<br/>167s]
    end
    
    S1 -.->|Escalation| S2
    S2 -.->|Escalation| S3
    S4 -.->|Escalation| S5
    S3 -.->|+| S5
    S5 -.->|Combined| S6
    
    style S1 fill:#90EE90
    style S2 fill:#FFD700
    style S3 fill:#FFA500
    style S4 fill:#87CEEB
    style S5 fill:#4682B4
    style S6 fill:#DC143C
```

**Recovery Time Progression:**
- Component failures: 41-55s
- Tier-specific: 71-103s  
- Complete system: 167s (max bound)

**Success Rate:** 100% with 10 retry attempts configured

**Evidence:** ~340 files, ~6 MB in `fault-tolerance-evidence/`

### Appendix D: Performance Benchmarks

| Metric | Value | Configuration |
|--------|-------|---------------|
| **End-to-end Latency (p95)** | 145ms | 4 parallel tasks |
| **Throughput** | ~10,000 events/s | 4 parallel tasks |
| **Window Processing** | 3s | Tumbling windows |
| **Checkpoint Duration** | <100ms | 36KB state |
| **Recovery Time (avg)** | 80.5s | Across S1-S6 |

### Appendix E: Quick Reference

**Start Local Development:**
```bash
./scripts/dev.sh start && ./scripts/dev.sh status
```

**Deploy to Kubernetes:**
```bash
./scripts/k8s.sh setup && ./scripts/k8s.sh deploy
```

**Run Fault Tolerance Tests:**
```bash
./scripts/k8s.sh test-ft all  # All 6 scenarios
```

**Access UIs:**
- Dashboard: http://localhost:3000
- Kafka UI: http://localhost:8080
- Flink UI (K8s): http://localhost:9081

**Monitor Status:**
```bash
./scripts/dev.sh status     # Local
./scripts/k8s.sh status     # Kubernetes
```

**View Logs:**
```bash
./scripts/dev.sh logs pricing-api          # Local service
./scripts/k8s.sh logs jobmanager           # Kubernetes component
```

---

## License & Credits

**Academic License**: Free to use, modify, and distribute for educational and research purposes.

**Commercial Use**: Not licensed for commercial deployment without proper security, compliance, and production hardening.

**Institution**: Instituto Tecnol√≥gico de Aeron√°utica (ITA)  
**Program**: Computer Engineering  
**Research Focus**: Fault-Tolerant Distributed Stream Processing

**Acknowledgments:**
- Apache Software Foundation (Flink, Kafka)
- Cloud Native Computing Foundation (Kubernetes)
- Spring Team (Spring Boot)
- PostgreSQL Global Development Group

---

## Project Status

**Version**: 2.0.0  
**Last Updated**: November 2025  
**Status**: ‚úÖ Production-Ready with Comprehensive Fault Tolerance Validation

**Test Coverage:**
- ‚úÖ 6 fault tolerance scenarios (100% system coverage)
- ‚úÖ ~340 evidence files for academic research
- ‚úÖ Disaster recovery validated (167s from 100% loss)
- ‚úÖ Configuration sensitivity quantified

**Academic Research Ready**: Complete evidence package for fault tolerance studies in distributed stream processing systems.
