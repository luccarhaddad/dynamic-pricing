---
# FlinkDeployment for Dynamic Pricing Job
# This resource defines the Flink job with High Availability enabled
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: pricing-job
  namespace: flink
  labels:
    app: dynamic-pricing
    component: flink-job
spec:
  # Flink image - should match the image with our application JAR
  # Note: Build and push this image before deploying
  image: flink-pricing-job:1.0.0
  flinkVersion: v1_17
  imagePullPolicy: IfNotPresent
  
  # ServiceAccount for HA (allows JobManager to access ConfigMaps and Leases)
  serviceAccount: flink
  
  # ============================================
  # FLINK CONFIGURATION
  # ============================================
  flinkConfiguration:
    # High Availability: Kubernetes-native HA using ConfigMap storage
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: s3://flink-ha/pricing-job
    
    # State Backend: HashMapStateBackend (in-memory state, persisted via checkpoints)
    state.backend: hashmap
    
    # Checkpoint Storage: S3/MinIO
    state.checkpoints.dir: s3://flink-checkpoints/pricing-job
    state.savepoints.dir: s3://flink-savepoints/pricing-job
    
    # Checkpoint Configuration (matches PricingJobMain.java defaults)
    execution.checkpointing.interval: 10s
    execution.checkpointing.mode: EXACTLY_ONCE
    execution.checkpointing.timeout: 10min
    execution.checkpointing.min-pause: 3s
    execution.checkpointing.max-concurrent-checkpoints: "1"
    execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
    execution.checkpointing.unaligned: "true"
    execution.checkpointing.tolerable-failed-checkpoints: "3"
    
    # Restart Strategy
    restart-strategy: fixed-delay
    restart-strategy.fixed-delay.attempts: "10"
    restart-strategy.fixed-delay.delay: 10s
    
    # Heartbeat Configuration
    heartbeat.timeout: "50000"
    heartbeat.interval: "10000"
    
    # TaskManager Configuration
    taskmanager.numberOfTaskSlots: "4"
    parallelism.default: "8"
    
    # Memory Configuration
    taskmanager.memory.process.size: 1024m
    jobmanager.memory.process.size: 1024m
    
    # Prometheus Metrics Configuration
    metrics.reporter.prometheus.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
    metrics.reporter.prometheus.port: "9249"
    metrics.reporters: prometheus
    
    # S3/MinIO Configuration
    s3.endpoint: http://minio.minio.svc.cluster.local:9000
    s3.access-key: minioadmin
    s3.secret-key: minioadmin
    s3.path.style.access: "true"
    s3.connection.ssl.enabled: "false"
  
  # ============================================
  # JOB MANAGER SPECIFICATION
  # ============================================
  jobManager:
    replicas: 2
    resource:
      memory: "1024m"
      cpu: 1
    
    # Pod template for JobManager
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          component: jobmanager
          app: pricing-job
      spec:
        containers:
          - name: flink-main-container
            ports:
            - containerPort: 9249
              name: metrics
              protocol: TCP
            env:
              # Application Configuration
              - name: FLINK_CHECKPOINT_INTERVAL
                value: "10000"
              - name: FLINK_MIN_PAUSE_BETWEEN_CHECKPOINTS
                value: "3000"
              - name: FLINK_CHECKPOINT_TIMEOUT
                value: "600000"
              - name: FLINK_PARALLELISM
                value: "8"
              - name: FLINK_WATERMARK_INTERVAL
                value: "10"
              - name: FLINK_ALLOWED_LATENESS
                value: "10"
              - name: FLINK_EVENT_TIME_WINDOW
                value: "5"
              
              # Kafka Configuration
              # For Minikube (Docker driver): use host.docker.internal (resolves to host gateway)
              # For Docker Desktop K8s: use host.docker.internal
              # For other K8s: use host IP or create a service
              - name: KAFKA_BROKERS
                value: "host.docker.internal:19092"
              - name: KAFKA_GROUP_ID
                value: "flink-pricing-job"
              - name: KAFKA_RIDE_REQUESTS_TOPIC
                value: "ride-requests"
              - name: KAFKA_DRIVER_HEARTBEATS_TOPIC
                value: "driver-heartbeats"
              - name: KAFKA_PRICE_UPDATES_TOPIC
                value: "price-updates"
              
              # S3/MinIO Configuration
              - name: CHECKPOINT_DIR
                value: "s3://flink-checkpoints/pricing-job"
              - name: S3_ENDPOINT
                value: "http://minio.minio.svc.cluster.local:9000"
              - name: S3_ACCESS_KEY
                value: "minioadmin"
              - name: S3_SECRET_KEY
                value: "minioadmin"
  
  # ============================================
  # TASK MANAGER SPECIFICATION
  # ============================================
  taskManager:
    replicas: 2  # Number of TaskManager pods (can scale horizontally)
    resource:
      memory: "1024m"
      cpu: 1
    
    # Pod template for TaskManager
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          component: taskmanager
          app: pricing-job
      spec:
        containers:
          - name: flink-main-container
            ports:
            - containerPort: 9249
              name: metrics
              protocol: TCP
            env:
              # Same environment variables as JobManager
              - name: FLINK_CHECKPOINT_INTERVAL
                value: "10000"
              - name: FLINK_MIN_PAUSE_BETWEEN_CHECKPOINTS
                value: "3000"
              - name: FLINK_CHECKPOINT_TIMEOUT
                value: "600000"
              - name: FLINK_PARALLELISM
                value: "8"
              - name: FLINK_WATERMARK_INTERVAL
                value: "10"
              - name: FLINK_ALLOWED_LATENESS
                value: "10"
              - name: FLINK_EVENT_TIME_WINDOW
                value: "5"
              
              # Kafka Configuration
              # For Minikube (Docker driver): use host.docker.internal (resolves to host gateway)
              # For Docker Desktop K8s: use host.docker.internal
              # For other K8s: use host IP or create a service
              - name: KAFKA_BROKERS
                value: "host.docker.internal:19092"
              - name: KAFKA_GROUP_ID
                value: "flink-pricing-job"
              - name: KAFKA_RIDE_REQUESTS_TOPIC
                value: "ride-requests"
              - name: KAFKA_DRIVER_HEARTBEATS_TOPIC
                value: "driver-heartbeats"
              - name: KAFKA_PRICE_UPDATES_TOPIC
                value: "price-updates"
              
              - name: CHECKPOINT_DIR
                value: "s3://flink-checkpoints/pricing-job"
              - name: S3_ENDPOINT
                value: "http://minio.minio.svc.cluster.local:9000"
              - name: S3_ACCESS_KEY
                value: "minioadmin"
              - name: S3_SECRET_KEY
                value: "minioadmin"
  
  # ============================================
  # JOB SPECIFICATION
  # ============================================
  job:
    # JAR file location (local:// means it's in the container image)
    jarURI: local:///opt/flink/usrlib/pricing-job.jar
    
    # Main class to execute
    entryClass: com.pricing.flink.PricingJobMain
    
    # Parallelism (should match FLINK_PARALLELISM env var)
    parallelism: 8
    
    # Job state: running (start immediately) or suspended (manual start)
    state: running
    
    # Upgrade mode: savepoint (zero-downtime upgrades) or stateless (no state)
    upgradeMode: savepoint
    
    # Allow non-restored state (set to false for strict recovery)
    allowNonRestoredState: false
    
    # Optional: Restore from specific savepoint/checkpoint
    # initialSavepointPath: s3://flink-savepoints/pricing-job/savepoint-abc123

